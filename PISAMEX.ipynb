{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Science 180\n",
    "\n",
    "\n",
    "\n",
    "![Alt text](http://thegeektown.com/wp-content/uploads/2015/03/data-scientist.jpg)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "An adventure through 15 dimensions of data wrangling, visualization and modeling at mind-bending speeds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## In a world with too much data sitting around and not enough insight, to whom will we turn for help?\n",
    "\n",
    "![Alt](http://nextviewventures.com/blog/wp-content/uploads/2014/07/control-content-marekting-for-startups.jpg)\n",
    "\n",
    "##YOU!    (Neo was busy...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### You must learn how to wrangle data in the next few hours in order to save the education system. If you fail, we're all doomed...\n",
    "\n",
    "### You have been given a dataset of test results and metadata, along with a laptop computer. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Good luck, everything depends on you.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###Where do we begin? \n",
    "\n",
    "\n",
    "###You know Python right? It does data stuff, right? OK, let's get started then.\n",
    "\n",
    "###First thing to figure out is how to get the data files on your machine into Python in the first place.\n",
    "\n",
    "\n",
    "###You've heard of this library called Pandas from another Agent -- it once saved them in a pinch. Lacking any better ideas, let's open up an editor and see if we can't at least cross the starting line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#Reading data from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "exam_data = pd.read_csv('data/PISA2009_Scored_Tests_MEX.csv')\n",
    "bio_data = pd.read_csv('data/PISA2009_Questionnaire_MEX.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###What did that just do? \n",
    "\n",
    "\n",
    "\n",
    "We called \"read_csv\", which presumably reads CSV files... and does what with them? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##pd.read_csv does a magical thing \n",
    "\n",
    "\n",
    "It reads a CSV file into a DataFrame. \n",
    "\n",
    "DataFrames are mystical creatures in Data Science. \n",
    "\n",
    "Popularized by R, they provide a standardized MATRIX-style format for interacting with your data. Most data can fit into this row and column format: financial transactions, iPhone app user records, medical histories, etc.\n",
    "\n",
    "(And you thought the Matrix references were just for fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt](http://www.bigdataexaminer.com/wp-content/uploads/2014/12/screen-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##Since you were wondering\n",
    "\n",
    "##Pandas has support for many formats\n",
    "\n",
    "CSV, Text (tab separated, pipe separated, etc.), Excel, JSON, HTML, SQL, Stuff copied to your clipboard, HDF5..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Slow down. What's really going on in the DataFrame?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Two data structures: Series and DataFrame\n",
    "\n",
    "###Series\n",
    "Think of this as one column of your data - one data type.\n",
    "\n",
    "### DataFrame\n",
    "All of the columns in your data. Mixed data types. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#Many Series can be combined and represented as a DataFrame object.\n",
    "\n",
    "#A DataFrame can be represented as many Series objects. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Pandas provides tons of functions to:\n",
    "\n",
    "###slice, dice, merge, join, group by, select, append, find, transform, sort, reverse, pivot and anything else you want to do\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####... for both Series and DataFrames. \n",
    "\n",
    "Most functions are designed to work with either type or even combinations of the two, just like you would intuitively expect:\n",
    "\n",
    "i.e. A concat function can contatenate arbitrary combinations of 0 to n Series and DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#So you know a bit about DataFrames?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "exam_data = pd.read_csv('data/PISA2009_Scored_Tests_MEX.csv')\n",
    "bio_data = pd.read_csv('data/PISA2009_Questionnaire_MEX.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Fine. What's next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#Exploratory Data Analysis. \n",
    "\n",
    "What the hell is in those files anyway?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it look like test data should? \n",
    "\n",
    "Is it completely empty? Full? Lots of missing values and NaN?\n",
    "\n",
    "What are in the rows? columns?\n",
    "\n",
    "Does it have appropriate features? (characteristics common to records belonging to a dataset)\n",
    "\n",
    "###It's impossible to make good decisions moving forward until we know more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can just output the entire dataframe to the console, but that doesn't scale beyond a couple hundred rows."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In [1]: df = DataFrame(np.random.randn(10, 4))\n",
    "\n",
    "In [2]: df\n",
    "Out[2]: \n",
    "          0         1         2         3\n",
    "0  0.469112 -0.282863 -1.509059 -1.135632\n",
    "1  1.212112 -0.173215  0.119209 -1.044236\n",
    "2 -0.861849 -2.104569 -0.494929  1.071804\n",
    "3  0.721555 -0.706771 -1.039575  0.271860\n",
    "4 -0.424972  0.567020  0.276232 -1.087401\n",
    "5 -0.673690  0.113648 -1.478427  0.524988\n",
    "6  0.404705  0.577046 -1.715002 -1.039268\n",
    "7 -0.370647 -1.157892 -1.344312  0.844885\n",
    "8  1.075770 -0.109050  1.643563 -1.469388\n",
    "9  0.357021 -0.674600 -1.776904 -0.968914"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#Pandas gives us a number of tools: \n",
    "\n",
    "\n",
    "    \n",
    "    .head(n)\n",
    "    .info()\n",
    "    .describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "exam_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "exam_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "exam_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#We have two files, and both of them have a feature named 'Student ID 5-digit'\n",
    "\n",
    "\n",
    "#Using this unique ID as our guide, we can match the exam scores and biographical data for a single student.\n",
    "\n",
    "#This task comes up a lot in data wrangling, since different kinds of data will be stored in different databases. Often one of the first steps is to combine the relevant parts of each part of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "useless = {\n",
    "    u' Version of cognitive database and date of release',\n",
    "    u'3-character country code ',\n",
    "    u'Adjudicated sub-region',\n",
    "}\n",
    "\n",
    "not_questions = {u'Booklet', \n",
    "                 u'School ID 5-digit', \n",
    "                 u'Student ID 5-digit',\n",
    "                 u'OECD country',\n",
    "                 u'Country code ISO 3-digit',}\n",
    "\n",
    "score_mapping = {\n",
    "    'Score 0': 0,\n",
    "    'Score 1': 1,\n",
    "    'Score 2': 2,\n",
    "    'Not reached': 0,\n",
    "}\n",
    "\n",
    "questions = set(exam_data.columns) - not_questions - useless\n",
    "\n",
    "for question in questions:\n",
    "    exam_data[question] = exam_data[question].map(score_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "math_qs = {q for q in questions if q.startswith('MATH')}\n",
    "read_qs = {q for q in questions if q.startswith('READ')}\n",
    "scie_qs = {q for q in questions if q.startswith('SCIE')}\n",
    "    \n",
    "totals = exam_data[list(questions)].sum(axis=1)\n",
    "math_score = exam_data[list(math_qs)].sum(axis=1)\n",
    "read_score = exam_data[list(read_qs)].sum(axis=1)\n",
    "scie_score = exam_data[list(scie_qs)].sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#Let's do a merge to get our data into a single managable file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "score_df = pd.DataFrame({'Total Score': totals, \n",
    "                         'Math Score': math_score, \n",
    "                         'Reading Score': read_score, \n",
    "                         'Science Score': scie_score,\n",
    "                         'Student ID 5-digit': exam_data['Student ID 5-digit']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.merge(score_df, bio_data, on='Student ID 5-digit')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Now we can pick any feature (read: column) and get information on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30 minutes or less a day      16997\n",
       "I donï¿½t read for enjoyment     9071\n",
       "Between 30 and 60 minutes      7047\n",
       "1 to 2 hours a day             3772\n",
       "More than 2 hours a day        1008\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Reading Enjoyment Time'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#But what about relationships between variables?\n",
    "\n",
    "We could look at sets of rows and see what occurs together, or compute statistics of co-occurrence ---> but still impossible to get a comprehensive view quickly.\n",
    "\n",
    "What can we try?\n",
    "\n",
    "#Let's do some plots!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#Matplotlib\n",
    "\n",
    "- The historical go-to for plotting\n",
    "- allows lots of fine-grained control\n",
    "- built with numpy in mind\n",
    "\n",
    "#Seaborn\n",
    "\n",
    "- Expressive power\n",
    "- built with pandas in mind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We will mainly use seaborn examples in this presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "###Jupyter notebooks can display graphics inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Blah blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#Scatterplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "View relationship between two continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sns.jointplot(data['Index of economic, social and cultural status (WLE)'], \n",
    "              data['Home Possessions'], kind=\"hex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this dataset, several variables can stand as proxies for socio-economic status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Histogram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Visualize distribution of continuous data.\n",
    "\n",
    "\n",
    "- Visualize distribution across categorical levels.\n",
    "\n",
    "- Can plot two histograms on top of each other\n",
    "\n",
    "- See the effects of the variable on the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "groups = data.groupby('Sex').groups\n",
    "for key, row_ids in groups.iteritems():\n",
    "    pylab.hist(data['Total Score'][row_ids].values,\n",
    "               normed=True,\n",
    "               bins=np.linspace(0, 70, 11),\n",
    "               alpha=0.35,\n",
    "               label=str(key))\n",
    "pylab.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###Doesn't work as well for more than two levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "groups = data.groupby('Grade').groups\n",
    "for key, row_ids in groups.iteritems():\n",
    "    pylab.hist(data['Total Score'][row_ids].values,\n",
    "               normed=True,\n",
    "               bins=np.linspace(0, 70, 11),\n",
    "               alpha=0.35,\n",
    "               label=str(key))\n",
    "pylab.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Violin Plots work better for comparing several distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "nonnull_subset = data['Total Score'].notnull()\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.violinplot(data['Total Score'][nonnull_subset], \n",
    "               data['Father  <Highest Schooling>'][nonnull_subset], \n",
    "               inner='box',\n",
    "               bw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##Alternatively, use `FacetGrid`\n",
    "###Visualize more effect of two variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(data, row=\"Sex\", col=\"At Home - Mother\", margin_titles=True)\n",
    "bins = np.linspace(0, 67, 13)\n",
    "g.map(plt.hist, \"Total Score\", color=\"steelblue\", bins=bins, lw=0, normed=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###If distribution not required, try factor plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "g = sns.factorplot(\"At Home - Father\", \"Total Score\", \"Sex\",\n",
    "                    data=data, kind=\"bar\",\n",
    "                    size=6, palette=\"muted\", dropna=True)\n",
    "g.despine(left=True)\n",
    "g.set_ylabels(\"Mean Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "g = sns.factorplot(\"Sex\", \"Total Score\", \"Sex\",\n",
    "                   row=\"At Home - Mother\",\n",
    "                   col=\"At Home - Father\",\n",
    "                   data=data, kind=\"bar\",\n",
    "                   size=6, palette=\"muted\",\n",
    "                   dropna=True)\n",
    "g.despine(left=True)\n",
    "g.set_ylabels(\"Mean Score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ptable = pd.pivot_table(\n",
    "    data, \n",
    "    values='Total Score', \n",
    "    index='Like Read - Fiction', \n",
    "    columns='Like Read - Non-fiction books')\n",
    "sns.heatmap(ptable, annot=True, fmt=\"f\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "###Not very useful if not in order..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###Heatmaps - Round 2\n",
    "####Effects of variables over a range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "display_order = [\n",
    "    u'Never or almost never',\n",
    "    u'A few times a year', \n",
    "    u'About once a month',                   \n",
    "    u'Several times a month', \n",
    "    u'Several times a week'\n",
    "]\n",
    "display_table = ptable[display_order].reindex(reversed(display_order))\n",
    "sns.heatmap(display_table,\n",
    "            annot=True, \n",
    "            fmt=\"f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###Pivot tables can do other aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "count_table = pd.pivot_table(\n",
    "    data, \n",
    "    values='Total Score', \n",
    "    index='Like Read - Fiction', \n",
    "    columns='Like Read - Non-fiction books',\n",
    "    aggfunc=np.count_nonzero)\n",
    "\n",
    "sns.heatmap(count_table[display_order].reindex(reversed(display_order)), annot=True, fmt=\"f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#Scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "some_sample = random.sample(data.index, 1000)\n",
    "sns.lmplot(\"Total Score\", \n",
    "           'Index of economic, social and cultural status (WLE)', \n",
    "           data.ix[some_sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Why plot just a subset of the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sns.lmplot(\"Total Score\", \n",
    "           'Index of economic, social and cultural status (WLE)', \n",
    "           data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bad_data = data['Reading Enjoyment Time'].apply(lambda x: isinstance(x, basestring) and x.startswith('I don'))\n",
    "reading_enjoyment = data['Reading Enjoyment Time'].copy()\n",
    "reading_enjoyment[bad_data] = 'No Joy'\n",
    "\n",
    "time_x_fiction = pd.pivot_table(\n",
    "    data, \n",
    "    values='Total Score', \n",
    "    index='Like Read - Fiction', \n",
    "    columns=reading_enjoyment)\n",
    "\n",
    "col_order = [\n",
    "    'No Joy', \n",
    "    '30 minutes or less a day', \n",
    "    'Between 30 and 60 minutes',\n",
    "    '1 to 2 hours a day',\n",
    "    'More than 2 hours a day'\n",
    "]\n",
    "\n",
    "row_order = [\n",
    "    'Several times a week',\n",
    "    'Several times a month',\n",
    "    'About once a month',\n",
    "    'A few times a year',\n",
    "    'Never or almost never',\n",
    "]\n",
    "\n",
    "display_table = time_x_fiction[col_order].reindex(row_order)\n",
    "sns.heatmap(display_table, annot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bad_data = data['Reading Enjoyment Time'].apply(lambda x: isinstance(x, basestring) and x.startswith('I don'))\n",
    "reading_enjoyment = data['Reading Enjoyment Time'].copy()\n",
    "reading_enjoyment[bad_data] = 'No Joy'\n",
    "\n",
    "time_x_nonfiction = pd.pivot_table(\n",
    "    data, \n",
    "    values='Total Score', \n",
    "    index='Like Read - Non-fiction books', \n",
    "    columns=reading_enjoyment)\n",
    "\n",
    "col_order = [\n",
    "    'No Joy', \n",
    "    '30 minutes or less a day', \n",
    "    'Between 30 and 60 minutes',\n",
    "    '1 to 2 hours a day',\n",
    "    'More than 2 hours a day'\n",
    "]\n",
    "\n",
    "row_order = [\n",
    "    'Several times a week',\n",
    "    'Several times a month',\n",
    "    'About once a month',\n",
    "    'A few times a year',\n",
    "    'Never or almost never',\n",
    "]\n",
    "\n",
    "non_f_display_table = time_x_nonfiction[col_order].reindex(row_order)\n",
    "sns.heatmap(non_f_display_table, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "key_reading = 'Online Reading'\n",
    "key_score = 'Total Score'\n",
    "key_econ = 'Index of economic, social and cultural status (WLE)'\n",
    "\n",
    "data_reading = data[key_reading]\n",
    "data_score = data[key_score]\n",
    "data_econ = data[key_econ]\n",
    "\n",
    "reading_bins = np.linspace(np.min(data_reading), np.max(data_reading), 5)\n",
    "econ_bins = np.linspace(np.min(data_econ), np.max(data_econ), 5)\n",
    "\n",
    "to_pivot = pd.DataFrame({\n",
    "    key_reading: np.digitize(data_reading, \n",
    "                             bins=reading_bins),\n",
    "    key_econ: np.digitize(data_econ,\n",
    "                          bins=econ_bins),\n",
    "    key_score: data_score\n",
    "})\n",
    "\n",
    "ptable = pd.pivot_table(\n",
    "    to_pivot, \n",
    "    values=key_score,\n",
    "    index=key_reading,\n",
    "    columns=key_econ,\n",
    "    aggfunc=np.mean)\n",
    "ptable.columns = pd.Series(map(str, econ_bins), name='Economic Status')\n",
    "ptable.index = pd.Series(map(str, reading_bins), name='Reading Values')\n",
    "\n",
    "sns.heatmap(ptable, annot=False, fmt=\"f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "\n",
    "# To look at the features again:\n",
    "#list(data.columns.values)\n",
    "print '****************'\n",
    "print \"What about missing values?\"\n",
    "print \"We can insert the mean!\"\n",
    "#data.mean()\n",
    "print '****************'\n",
    "good_rows = data[['Home Possessions','Grade', 'Grade compared to modal grade in country', 'Math Score', 'Reading Score', 'Total Score']].fillna(data.mean())\n",
    "\n",
    "#good_rows.describe()\n",
    "#good_rows.info()\n",
    "\n",
    "new_data = good_rows[['Home Possessions','Grade', 'Grade compared to modal grade in country', 'Math Score', 'Reading Score']]\n",
    "\n",
    "training_target = good_rows['Total Score'][:-5000]\n",
    "training = new_data[:-5000]\n",
    "\n",
    "validation_target = good_rows['Total Score'][-5000:]\n",
    "validation = new_data[-5000:]\n",
    "\n",
    "lr = linear_model.LinearRegression()\n",
    "\n",
    "lr.fit(training, training_target)\n",
    "\n",
    "# #pd.unique(good_rows.values.ravel())\n",
    "# print \"NUll: {0} \\n\".format(good_rows.isnull().sum())\n",
    "# print \"***************\"\n",
    "# predicted = sklearn.cross_validation.cross_val_predict(linear_model.LinearRegression(), new_data, target, cv=2)\n",
    "\n",
    "# fig,ax = matplotlib.pyplot.subplots()\n",
    "# ax.scatter(target, predicted)\n",
    "# fig.show()\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', lr.coef_)\n",
    "# The root mean square error\n",
    "print(\"RMSE: %.2f\"\n",
    "      % np.sqrt(np.mean(lr.predict(validation) - validation_target) ** 2))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % lr.score(validation, validation_target))\n",
    "\n",
    "# Plot outputs\n",
    "print validation.shape, validation_target.shape\n",
    "\n",
    "lr_output = pd.concat([pd.DataFrame(lr.predict(validation), columns=['Predicted']), validation_target], axis=1)\n",
    "lr_output.info()\n",
    "\n",
    "matplotlib.pyplot.scatter(lr.predict(validation), validation_target,  color='red')\n",
    "matplotlib.pyplot.plot([0,60], [0,60], color='blue',\n",
    "         linewidth=3)\n",
    "\n",
    "#matplotlib.pyplot.xticks(())\n",
    "#matplotlib.pyplot.yticks(())\n",
    "\n",
    "matplotlib.pyplot.show()\n",
    "\n",
    "#sns.jointplot('Predicted', 'Total Score', data=lr_output, kind=\"reg\",\n",
    "#                  xlim=(0, 60), ylim=(0, 20), color=color, size=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sklearn.metrics.log_loss(target, predicted)\n",
    "print \"look at log_loss?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC(gamma=0.001, C=100.)\n",
    "clf.fit(training, training_target)\n",
    "\n",
    "# fig1,ax1 = matplotlib.pyplot.subplots()\n",
    "# ax1.scatter(target,clf.predict(new_data))\n",
    "# fig1.show()\n",
    "#target\n",
    "clf_predict = clf.predict(validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#clf_predicted_actual = pd.concat([clf_predict, target], axis=1, keys=['predicted', 'actual'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "color = sns.color_palette()[2]\n",
    "#g = sns.jointplot('predicted', 'actual', data=clf_predicted_actual, kind=\"reg\",\n",
    "#                  xlim=(0, 60), ylim=(0, 20), color=color, size=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create default Random Forest Regression Model\n",
    "from sklearn.ensemble.forest import RandomForestRegressor\n",
    "rf = RandomForestRegressor()\n",
    "rf.fit(training, training_target)\n",
    "\n",
    "\n",
    "rf_predictions = pd.DataFrame(rf.predict(validation), columns=['Predicted Math Score'])\n",
    "rf_validation_target = pd.DataFrame(validation_target, columns=['Total Score']).reset_index()\n",
    "rf_predicted_actual = pd.concat([rf_predictions, rf_validation_target], axis=1)\n",
    "print rf_predicted_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "color = sns.color_palette()[1]\n",
    "g = sns.jointplot('Predicted Math Score', 'Total Score', data=rf_predicted_actual, kind=\"reg\",\n",
    "                  xlim=(0, 25), ylim=(0, 25), color=color, size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = rf.score(validation, validation_target)\n",
    "print \"Score: {}\".format(score)\n",
    "print rf.feature_importances_, new_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "colors = ['','red','blue','green','red','blue','green']\n",
    "for degree in [1, 2, 6]:\n",
    "    model = make_pipeline(PolynomialFeatures(degree), Ridge())\n",
    "    model.fit(training, training_target)\n",
    "    y_plot = model.predict(validation)\n",
    "    plt.scatter(validation_target, y_plot, color=colors[degree],label=\"degree %d\" % degree)\n",
    "    matplotlib.pyplot.plot([0,60], [0,60], color='black',linewidth=3)\n",
    "    matplotlib.pyplot.legend(loc='lower left')\n",
    "    matplotlib.pyplot.show()\n",
    "    # The mean square error\n",
    "    print(\"RMSE: %.2f\" % np.sqrt(np.mean((y_plot - validation_target) ** 2)))\n",
    "    # Explained variance score: 1 is perfect prediction\n",
    "    print('Variance score: %.2f' % model.score(validation, validation_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fit regression model\n",
    "svr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
    "svr_lin = SVR(kernel='linear', C=1e3)\n",
    "svr_poly = SVR(kernel='poly', C=1e3, degree=2)\n",
    "y_rbf = svr_rbf.fit(training, training_target).predict(validation)\n",
    "y_lin = svr_lin.fit(training, training_target).predict(validation)\n",
    "y_poly = svr_poly.fit(training, training_target).predict(validation)\n",
    "\n",
    "plt.scatter(training, training_target, c='k', label='data')\n",
    "plt.hold('on')\n",
    "plt.plot(training, y_rbf, c='g', label='RBF model')\n",
    "plt.plot(training, y_lin, c='r', label='Linear model')\n",
    "plt.plot(training, y_poly, c='b', label='Polynomial model')\n",
    "plt.xlabel('data')\n",
    "plt.ylabel('target')\n",
    "plt.title('Support Vector Regression')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#Classifcation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\"How do we do that?\"\n",
    "\"Our target becomes a discrete variable\"\n",
    "\"Our training data can stay the same\""
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
